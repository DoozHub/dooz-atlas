# LLM Vibe Check

> Pragmatic Model Evaluation for Real-World Software Development

---

## Overview

An open-source, community-driven experiment by **DoozieSoft** to qualify LLMs for real-world software agency work. Unlike academic benchmarks, this tests LLMs on practical, messy scenarios from actual tickets.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM VIBE CHECK                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ§ª Pragmatic Tests     â”‚  ðŸ“Š Real-World Scenarios          â”‚
â”‚  ðŸ¤– Model Comparison    â”‚  ðŸ“ Prompt/Result Pairs           â”‚
â”‚  ðŸŽ¯ Code Refactoring    â”‚  ðŸ’¬ Client Communication          â”‚
â”‚  ðŸ”§ Bug Detection       â”‚  ðŸ‘¥ Community Driven              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Philosophy

> **Progress over Perfection**

DoozieSoft doesn't need models that solve theoretical physics. They need partners that:
- Refactor messy code without over-engineering
- Communicate with clients like humans
- Find that one annoying VPC setting crashing an app

---

## How It Works

1. **Prompts**: High-friction scenarios from real DoozieSoft tickets
2. **Execution**: Manual model switching with GitHub Copilot or Kilo
3. **Results**: Store outputs in `/results/` for comparison

---

## Repository Structure

```
llm-vibe-check/
â”œâ”€â”€ .kilocode/            # Custom persona for Kilo users
â”œâ”€â”€ prompts/              # Pragmatic scenarios (Input)
â”œâ”€â”€ results/              # Model-generated responses (Output)
â”‚   â”œâ”€â”€ gpt-4o/
â”‚   â””â”€â”€ gemini-2.0-flash/
â”œâ”€â”€ Master Execution Prompt.md
â”œâ”€â”€ CONTRIBUTING.md
â””â”€â”€ RESULTS.md
```

---

## Documentation

| Document | Description |
|----------|-------------|
| `Master Execution Prompt.md` | Main evaluation prompt |
| `RESULTS.md` | Model comparison results |
| `CONTRIBUTING.md` | How to add new prompts |

---

## Contributing

This is a living experiment. Add prompts when:
- An LLM gives a brilliant solution â†’ **Add it**
- An LLM fails at a simple task â†’ **Add that too**

---

*Repository: llm-vibe-check*
